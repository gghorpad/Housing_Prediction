---
title: "Housing Prediction Documentation"
author: "Gaurav"
date: "28 February 2019"
output:
  pdf_document: default
  html_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

###Background
This project involves **Sales Price prediction** for houses in Ames, Iowa.  
The data has **79 variables** describing the different aspects of the houses.  
The dataset comes from [this kaggle competition.](https://www.kaggle.com/c/house-prices-advanced-regression-techniques#description)

[**Github Repository**](https://github.com/gghorpad/Housing_Prediction.git)

**Note:** This Rmd File requires the workspace generated by the script. The script requires the dataset either from kaggle or the github repo. 

**Evaluation Metric**
The result is evaluated on the basis of **RMSE** between the **logarithm** of the predicted price and actual price.


###Introduction
The competition gives us both a training and a test set. The test set gives us an RMSE when uploaded online.  
For quick testing, I partitioned the training data into a training subset and validation set with 75% in the training. The true values of SalePrice for test set are not known but prediction performance can be checked by uploading predictions online.

KeySteps:
1. Read in Data and Explore
2. Impute missing Values
3. Choose most relevant columns and engineer New Features
4. Fit models on training subset and validation subset using different approaches
5. Evaluate Results and select promising approaches
6. Fit models on entire training data and predict final outcome
7. Upload to Kaggle and finalize methodolgy

```{r dataread, include=FALSE,echo=FALSE}
library(tidyverse)
library(caret)

train<-read.csv("train.csv")
test<-read.csv("test.csv")

print(paste("The Dimensions of training dataset are",length(train[,1]),"by",length(train[1,])))
print(paste("The Dimensions of test dataset are",length(test[,1]),"by",length(test[1,])))

set.seed(2)

RMSE<-function(predicted_price,actual_price){
  sqrt(mean((predicted_price-actual_price)^2))}
```
The explanations for the columns are available in a separate data description file. (Available on Kaggle and The Github Rep)

###Preprocessing

There are three types of data in the columns, categorical, ordinal and numeric. Their treatment is explained below with examples. 

**Categorical Data**

MSZoning: Identifies the general zoning classification of the sale.
		
       A	Agriculture
       C	Commercial
       FV	Floating Village Residential
       I	Industrial
       RH	Residential High Density
       RL	Residential Low Density
       RP	Residential Low Density Park 
       RM	Residential Medium Density

To use this type of data effectively with machine learning algorithms, we will encode it into boolean data. One column will be created for each category. Each column will identify whether that row corresponds to that category. 

This will be done for each of the columns in original data and we will finally end up with many columns. Since it is not feasible to manually do this for each variable, it will be achieved using dynamic variable names.

**Ordinal Data**

ExterQual: Evaluates the quality of the material on the exterior 
	
       Ex	Excellent
       Gd	Good
       TA	Average/Typical
       Fa	Fair
       Po	Poor

This data has natural ordering and is converted into a column of corresponding numerical values. That is "Po"=0 , "Fa"=1 etc.

**Numeric Data**

GrLivArea: Above grade (ground) living area square feet

This type of data is generally used as is. However for neural-networks, these attributes were centered and scaled. 

####Imputation of Missing Values

I replace some of the missing values with the most common value when one value occurs much more frequently than others. 

```{r, echo=FALSE,warning=FALSE,fig.height = 3, fig.width = 5, fig.align = "center"}

train%>%ggplot(aes(x=MasVnrArea,y=SalePrice))+geom_point()+scale_y_continuous(labels = scales::comma)+ggtitle("Masonry Veneer Area Distribution")+theme(plot.title = element_text(size = 12, face = "bold",hjust=0.5))
```


```{r miss2}
train$MasVnrArea[is.na(train$MasVnrArea)]<-0
train%>%group_by(SaleType)%>%summarize(count=n())
train$SaleType[is.na(train$SaleType)]<-"WD"
train%>%group_by(Functional)%>%summarize(count=n())
train$Functional[is.na(train$Functional)]<-"Typ"
train%>%group_by(Exterior1st)%>%summarize(count=n())
train$Exterior1st[is.na(train$Exterior1st)]<-"VinylSd"
train%>%group_by(MSZoning)%>%summarize(count=n())
train$MSZoning[is.na(train$MSZoning)]<-"RL"
train%>%group_by(Electrical)%>%summarize(count=n())
train[is.na(train$Electrical),"Electrical"]<-"SBrkr"
```

In some cases I suspect values are missing because condition is Not Applicable E.g: Garage Area is missing because Garage does not exist etc.
```{r miss3}
train$GarageArea[is.na(train$GarageArea)]<-0
train$GarageCars[is.na(train$GarageCars)]<-0
train[is.na(train$TotalBsmtSF),c("TotalBsmtSF","BsmtFinSF1","BsmtFinSF2","BsmtUnfSF")]<-0
train[is.na(train$BsmtFullBath),c("BsmtFullBath","BsmtHalfBath")]<-0
```

```{r LotFrontageImpute,fig.show='hold',fig.align='center',warning=FALSE,echo=FALSE}
#train%>%ggplot(aes(y=LotFrontage,x=LotArea))+geom_point()+geom_smooth(method="loess")
train%>%filter(LotArea<38000)%>%ggplot(aes(y=LotFrontage,x=LotArea))+geom_point(aes(color=LotShape))+geom_smooth(method="loess")+ggtitle("Impute LotFrontage")

```

LotFrontage: Linear feet of street connected to property  
There are a large number of missing values for LotFrontage.  
I therefore fit a curve to estimate LotFrontage from LotArea. These variables are highly correlated. The different LotShapes do not have an obviously different ratio and LotShape was not used to improve the estimate.

```{r LFI,echo=FALSE}
train_temp<-train
fit_LotAreaVsFrontage<-loess(LotFrontage~LotArea,data=train_temp)
train_missing<-train[is.na(train$LotFrontage),]
train_missing_predict<-predict(fit_LotAreaVsFrontage,train_missing)
train[is.na(train$LotFrontage),"LotFrontage"]<-train_missing_predict

```


####Skewness of Target Variable


```{r, echo=FALSE,fig.height = 4, fig.width = 6.25, fig.align = "center"}
train%>%ggplot(aes(SalePrice))+geom_density()+scale_x_continuous(labels = scales::comma)+ggtitle("Smooth Density Plot of SalePrice")
```

To account for this we take the log of the SalePrice and store it in a variable called LSP

```{r,echo=FALSE,fig.height = 4, fig.width = 6.25, fig.align = "center"}
train%>%ggplot(aes(log(SalePrice)))+geom_density()+ggtitle("Smooth Density Plot of Logarithmic SalePrice")
train1<-train%>%mutate(LSP=log(SalePrice))

```

###Data Exploration

In this section I looked through the columns one by one. I used this to decide which variables could be useful and which were not. This is also the part where I converted the categorical data into separate features.

```{r DataExplore,warning=FALSE,echo=FALSE,fig.height = 4, fig.width = 6.25, fig.align = "center"}
load("HarvardX_Workspace_Final_GG.RData")
train%>%ggplot(aes(y=LSP,x=LotShape))+geom_boxplot()+ggtitle("LotShape vs Log SalePrice")
train%>%ggplot(aes(y=LSP,x=BLotShape))+geom_boxplot()+ggtitle("New LotShape Feature")+labs(subtitle="LotShape is Regular? T/F")


train%>%ggplot(aes(y=LSP,x=LotConfig))+geom_boxplot()+ggtitle("LotConfig vs Log SalePrice")
train%>%ggplot(aes(y=LSP,x=BLotConfig))+geom_boxplot()+ggtitle("New LotConfig Feature")+labs(subtitle="LotConfig is CulDSac+Fr3? T/F")

train%>%ggplot(aes(y=LSP,x=Street))+geom_boxplot()+ggtitle("Street vs LSP")+labs(subtitle = "Street is boolean")

train%>%ggplot(aes(y=LSP,x=Alley))+geom_boxplot()+ggtitle("Alley vs Log SalePrice")

train%>%ggplot(aes(y=LSP,x=LandContour))+geom_boxplot()+ggtitle("LandContour vs Log SalePrice")

train%>%ggplot(aes(y=LSP,x=Utilities))+geom_boxplot()+ggtitle("Utilities vs LSP")+labs(subtitle="There is only 1 NoSewage entry")


train%>%ggplot(aes(y=LSP,x=LandSlope))+geom_boxplot()+ggtitle("LandSlope vs Log SalePrice")

train%>%ggplot(aes(y=LSP,x=Condition1))+geom_boxplot()+ggtitle("Condition1 vs Log SalePrice")

train%>%ggplot(aes(y=LSP,x=Condition2))+geom_boxplot()+ggtitle("Condition2 vs Log SalePrice")

train%>%ggplot(aes(y=LSP,x=Neighborhood))+geom_boxplot()+ggtitle("Neighborhood vs Log SalePrice")+ theme(axis.text.x = element_text(angle = 90, hjust = 1))

train%>%ggplot(aes(y=LSP,x=BldgType))+geom_boxplot()+ggtitle("BldgType vs Log SalePrice")

train%>%ggplot(aes(y=LSP,x=HouseStyle))+geom_boxplot()+ggtitle("HouseStyle vs Log SalePrice")

train%>%ggplot(aes(y=LSP,x=RoofStyle))+geom_boxplot()+ggtitle("RoofStyle vs LSP")+labs(subtitle="RoofStyle Effect is weak, is dropped")


train%>%ggplot(aes(y=LSP,x=Exterior1st))+geom_boxplot()+ggtitle("Exterior 1st vs Log SalePrice")+ theme(axis.text.x = element_text(angle = 90, hjust = 1))

train%>%ggplot(aes(y=Exterior1st,x=Exterior2nd))+geom_count()+ggtitle("Exterior 2nd vsExterior 1st")+ theme(axis.text.x = element_text(angle = 90, hjust = 1))+labs(subtitle="Exterior 2nd is similar to Exterior 1st")

train%>%ggplot(aes(y=LSP,x=MasVnrType))+geom_boxplot()+ggtitle("Masonry Veneer Type vs Log SalePrice")

train%>%ggplot(aes(y=LSP,x=MasVnrArea))+geom_point()+ggtitle("Masonry Veneer Area vs LSP")+labs(subtitle="MasVnrArea is not strongly correlated to LSP, is dropped")

train%>%ggplot(aes(y=LSP,x=Foundation))+geom_boxplot()+ggtitle("Foundation vs Log SalePrice")

train%>%ggplot(aes(y=LSP,x=Heating))+geom_boxplot()+ggtitle("Heating vs Log SalePrice")

train%>%ggplot(aes(y=LSP,x=Electrical))+geom_boxplot()+ggtitle("Electrical vs Log SalePrice")

train%>%ggplot(aes(y=LSP,x=Fireplaces))+geom_point()+ggtitle("#Fireplaces vs LSP")

train%>%ggplot(aes(y=LSP,x=Functional))+geom_boxplot()+ggtitle("Functional vs Log SalePrice")

train%>%ggplot(aes(y=LSP,x=FireplaceQu))+geom_boxplot()+ggtitle("FirePlace Quality vs Log SalePrice")

train%>%ggplot(aes(y=LSP,x=GarageType))+geom_boxplot()+ggtitle("GarageType vs Log SalePrice")

train%>%ggplot(aes(y=LSP,x=SaleType))+geom_boxplot()+ggtitle("SaleType vs Log SalePrice")

train%>%ggplot(aes(y=LSP,x=SaleCondition))+geom_boxplot()+ggtitle("SaleCondition vs Log SalePrice")

train%>%ggplot(aes(y=LSP,x=CentralAir))+geom_boxplot()+ggtitle("Central Air Conditioning vs Log SalePrice")

train<-train%>%mutate(BCentralAir=CentralAir=="Y")

train%>%ggplot(aes(y=LSP,x=MoSold))+geom_point()+labs(subtitle="Month Effect is not strong, MoSold is dropped")+ggtitle("Month Sold vs LSP")

```


####Outliers

There are a few Outliers in data which may impact the model fit. These 4 values are removed before further analysis. Online documentation by author of this dataset confirmed that these data points may not be representative and should be discarded.  

```{r outliers,echo=FALSE,warning=FALSE}
cols <- c("TRUE" = "red", "FALSE"="black")
train1%>%mutate(Outlier=GrLivArea>4000)%>%ggplot(aes(x=GrLivArea,y=LSP))+geom_point(aes(color=Outlier))+ggtitle("Outliers in data")+ scale_colour_manual(values = cols)
```
  
  
###Data Analysis and Results

Several Different Models were fit on training subset.  
Of these following Models resulted in promising results:  
  1. Linear Model   RMSE= 0.1188609  
  2. Random Forest  RMSE= 0.1229782  
  3. GBM            RMSE= 0.1156895    
  4. BRNN           RMSE= 0.1184115  

Averaging the result of these four models resulted in an incremental improvemnt in result.
The RMSE of average prediction for above 4 models is 0.1089506 on validation subset.

The final RMSE on test subset from kaggle is **0.12570**.
This score puts the answer at about rank 1500/4100+ teams on Kaggle.

A further improvement in score(small) is possible by replacing BRNN with QRNN. Since QRNN takes significantly longer to train, I have left it out of the report.

###Observations and conclusions
The RMSE from validation is slightly lower than that from the test set reported by kaggle. This indicates a slight overfit. In this regard, the Random Forest is closest to the actual value indicating its robustness to overfitting. 

The Linear Model was surprisingly effective. In a real world scenario this is probably the best model I would use because it would be easy to interpret and use in a practical scenario. It is possible that on the ground, humans tend to value houses in some approximation of a linear model i.e. paying a fixed price per Sq.Ft depending on quality and locality etc. This may explain why the linear model works well.

For example from the model, it seems that a metal siding is worth about 20% more than wood siding for the Exterior which is covering the house. 

From the Tree Fit we can get the variable importance.
We can see that the most important factors are the Overall Quality and the Above ground Living Area.